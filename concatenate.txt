
# Start of env.py
"""Environment Manipulation."""

import json
import os
import random

import numpy as np
from PIL import Image

from node import Node
from task import Task
from schedule import CompactScheduler
from schedule import SpreadScheduler
from schedule import DeepRMScheduler


class Environment:
    """Environment Simulation."""

    def __init__(self, nodes, queue_size, backlog_size, task_generator):
        self.nodes = nodes
        self.queue_size = queue_size
        self.backlog_size = backlog_size
        self.queue = []
        self.backlog = []
        self.timestep_counter = 0
        self._task_generator = task_generator
        self._task_generator_end = False

    def timestep(self):
        """Proceed to the next timestep."""
        self.timestep_counter += 1

        # each node proceeds to the next timestep
        for node in self.nodes:
            node.timestep()

        # move tasks from backlog to queue
        p_queue = len(self.queue)
        p_backlog = 0
        indices = []
        while p_queue < self.queue_size and p_backlog < len(self.backlog):
            self.queue.append(self.backlog[p_backlog])
            indices.append(p_backlog)
            p_queue += 1
            p_backlog += 1
        for i in sorted(indices, reverse=True):
            del self.backlog[i]

        # accept more tasks, move to backlog
        p_backlog = len(self.backlog)
        while p_backlog < self.backlog_size:
            new_task = next(self._task_generator, None)
            if new_task is None:
                self._task_generator_end = True
                break
            else:
                self.backlog.append(new_task)
                p_backlog += 1

    def terminated(self):
        """Check environment termination."""
        for node in self.nodes:
            if node.utilization() > 0:
                return False
        if self.queue or self.backlog or not self._task_generator_end:
            return False
        return True

    def reward(self):
        """Reward calculation."""
        r = 0
        for node in self.nodes:
            if node.scheduled_tasks:
                r += 1/sum([task[0].duration for task in node.scheduled_tasks])
        if self.queue:
            r += 1/sum([task.duration for task in self.queue])
        if self.backlog:
            r += 1/sum([task.duration for task in self.backlog])
        return -r

    def summary(self, bg_shape=None):
        """State representation."""
        # background shape
        if bg_shape is None:
            bg_col = max([max(node.resources) for node in self.nodes])
            bg_row = max([node.duration for node in self.nodes])
            bg_shape = (bg_row, bg_col)

        if len(self.nodes) > 0:
            dimension = self.nodes[0].dimension

            # state of nodes
            temp = self.nodes[0].summary(bg_shape)
            for i in range(1, len(self.nodes)):
                temp = np.concatenate((temp, self.nodes[i].summary(bg_shape)), axis=1)

            # state of occupied queue slots
            for i in range(len(self.queue)):
                temp = np.concatenate((temp, self.queue[i].summary(bg_shape)), axis=1)

            # state of vacant queue slots
            empty_summary = Task([0]*dimension, 0, 'empty_task').summary(bg_shape)
            for i in range(len(self.queue), self.queue_size):
                temp = np.concatenate((temp, empty_summary), axis=1)

            # state of backlog
            backlog_summary = Task([0], 0, 'empty_task').summary(bg_shape)
            p_backlog = 0
            p_row = 0
            p_col = 0
            while p_row < bg_shape[0] and p_col < bg_shape[1] and p_backlog < len(self.backlog):
                backlog_summary[p_row, p_col] = 0
                p_row += 1
                if p_row == bg_shape[0]:
                    p_row = 0
                    p_col += 1
                p_backlog += 1
            temp = np.concatenate((temp, backlog_summary), axis=1)

            return temp
        else:
            return None

    def plot(self, bg_shape=None):
        """Plot state representation into image."""
        if not os.path.exists('__cache__/state'):
            os.makedirs('__cache__/state')
        summary_matrix = self.summary(bg_shape)
        summary_plot = np.full((summary_matrix.shape[0], summary_matrix.shape[1]), 255, dtype=np.uint8)
        for row in range(summary_matrix.shape[0]):
            for col in range(summary_matrix.shape[1]):
                summary_plot[row, col] = summary_matrix[row, col]
        Image.fromarray(summary_plot).save('__cache__/state/environment_{0}.png'.format(self.timestep_counter))

    def __repr__(self):
        return 'Environment(timestep_counter={0}, nodes={1}, queue={2}, backlog={3})'.format(self.timestep_counter, self.nodes, self.queue, self.backlog)


def load(load_environment=True, load_scheduler=True):
    """Load environment and scheduler from conf/env.conf.json"""
    tasks = _load_tasks()
    task_generator = (t for t in tasks)
    with open('conf/env.conf.json', 'r') as fr:
        data = json.load(fr)
        nodes = []
        label= 0
        for node_json in data['nodes']:
            label += 1
            nodes.append(Node(node_json['resource_capacity'], node_json['duration_capacity'], 'node' + str(label)))
        environment = None
        scheduler = None
        if load_environment:
            environment = Environment(nodes, data['queue_size'], data['backlog_size'], task_generator)
            environment.timestep()
        if load_scheduler:
            if 'CompactScheduler' == data['scheduler']:
                scheduler = CompactScheduler(environment)
            elif 'SpreadScheduler' == data['scheduler']:
                scheduler = SpreadScheduler(environment)
            else:
                scheduler = DeepRMScheduler(environment, False)
        return (environment, scheduler)


def _load_tasks():
    """Load tasks from __cache__/tasks.csv"""
    _generate_tasks()
    tasks = []
    with open('__cache__/tasks.csv', 'r') as fr:
        resource_indices = []
        duration_index = 0
        label_index = 0
        line = fr.readline()
        parts = line.strip().split(',')
        for i in range(len(parts)):
            if parts[i].strip().startswith('resource'):
                resource_indices.append(i)
            if parts[i].strip() == 'duration':
                duration_index = i
            if parts[i].strip() == 'label':
                label_index = i
        line = fr.readline()
        while line:
            parts = line.strip().split(',')
            resources = []
            for index in resource_indices:
                resources.append(int(parts[index]))
            tasks.append(Task(resources, int(parts[duration_index]), parts[label_index]))
            line = fr.readline()
    return tasks


def _generate_tasks():
    """Generate tasks according to conf/task.pattern.conf.json"""
    if not os.path.exists('__cache__'):
        os.makedirs('__cache__')
    if os.path.isfile('__cache__/tasks.csv'):
        return
    with open('conf/task.pattern.conf.json', 'r') as fr, open('__cache__/tasks.csv', 'w') as fw:
        data = json.load(fr)
        if len(data) > 0:
            for i in range(len(data[0]['resource_range'])):
                fw.write('resource' + str(i+1) + ',')
            fw.write('duration,label' + '\n')
        label = 0
        for task_pattern in data:
            for i in range(task_pattern['batch_size']):
                label += 1
                resources = []
                duration = str(random.randint(task_pattern['duration_range']['lowerLimit'], task_pattern['duration_range']['upperLimit']))
                for j in range(len(task_pattern['resource_range'])):
                    resources.append(str(random.randint(task_pattern['resource_range'][j]['lowerLimit'], task_pattern['resource_range'][j]['upperLimit'])))
                fw.write(','.join(resources) + ',' + duration +  ',' + 'task' + str(label) + '\n')

# End of env.py

# Start of FL_AVG.py
from schedule import *

def combine_agents(main_agent, agents):
    for i in range(len(agents)):
        for main_param, agent_param in zip(main_agent.dqn_train.model.trainable_variables, agents[i].dqn_train.model.trainable_variables):
            if (i == 0):
                main_param.assign(agent_param)
            else:
                main_param.assign(main_param * (i / (i + 1)) + agent_param * (1 / (i + 1)))

        for main_param, agent_param in zip(main_agent.dqn_target.model.trainable_variables, agents[i].dqn_target.model.trainable_variables):
            if (i == 0):
                main_param.assign(agent_param)
            else:
                main_param.assign(main_param * (i / (i + 1)) + agent_param * (1 / (i + 1)))

    return main_agent

def combine_agents_reward_based(main_agent, agents, scores):
    total_reward = sum(scores)

    for i in range(len(agents)):
        for main_param, agent_param in zip(main_agent.dqn_train.model.trainable_variables, agents[i].dqn_train.model.trainable_variables):
            if i == 0:
                main_param.assign(agent_param * (scores[i] / total_reward))
            else:
                main_param.assign(main_param + agent_param * (scores[i] / total_reward))

        for main_param, agent_param in zip(main_agent.dqn_target.model.trainable_variables, agents[i].dqn_target.model.trainable_variables):
            if i == 0:
                main_param.assign(agent_param * (scores[i] / total_reward))
            else:
                main_param.assign(main_param + agent_param * (scores[i] / total_reward))

    return main_agent

def distribute_agents(main_agent, agents):
    for i in range(len(agents)):
        for main_agent_param, agent_param in zip(main_agent.dqn_target.model.trainable_variables,  agents[i].dqn_target.model.trainable_variables):
            agent_param.assign(main_agent_param)
        for main_agent_param, agent_param in zip(main_agent.dqn_train.model.trainable_variables, agents[i].dqn_train.model.trainable_variables):
            agent_param.assign(main_agent_param)
    return agents


# End of FL_AVG.py

# Start of concatenate.py
import os

def concatenate_python_files(source_directory, output_file):
    """
    Concatenates all Python files in the specified directory into a single file.

    Args:
    source_directory (str): The path to the directory containing Python files.
    output_file (str): The path to the output file where the concatenated result will be stored.
    """
    # Ensure the directory exists
    if not os.path.exists(source_directory):
        print(f"Error: The directory {source_directory} does not exist.")
        return

    # Create or overwrite the output file
    with open(output_file, 'w', encoding='utf-8') as outfile:
        # Walk through the directory
        for dirpath, dirnames, filenames in os.walk(source_directory):
            for filename in filenames:
                if filename.endswith('.py'):
                    # Path to the current file
                    file_path = os.path.join(dirpath, filename)
                    # Writing the name of the file as a comment
                    outfile.write(f"\n# Start of {filename}\n")
                    # Open and read the current file
                    with open(file_path, 'r', encoding='utf-8') as infile:
                        outfile.write(infile.read())
                    # Writing the end of the file as a comment
                    outfile.write(f"\n# End of {filename}\n")

# Example usage
if __name__ == "__main__":
    source_dir = "./"  # Change to the path of your source directory
    output_path = "./concatenate.txt"  # Change to your desired output file path
    concatenate_python_files(source_dir, output_path)

# End of concatenate.py

# Start of node.py
"""Node Manipulation."""

import collections
import os

import numpy as np
from PIL import Image


class Node:
    """Node."""

    def __init__(self, resources, duration, label):
        self.resources = resources
        self.duration = duration
        self.label = label
        self.dimension = len(resources)
        self.timestep_counter = 0
        self.scheduled_tasks = []
        # state matrices, 0 stands for occupied slot, 255 stands for vacant slot
        self.state_matrices = [np.full((duration, resource), 255, dtype=np.uint8) for resource in resources]
        self._state_matrices_capacity = [[resource]*duration for resource in resources]

    def schedule(self, task):
        """Schedule task on node."""
        # find the earliest time to schedule task
        start_time = self._satisfy(self._state_matrices_capacity, task.resources, task.duration)

        if start_time == -1:
            # failure, no capacity
            return False
        else:
            # success, update node state
            for i in range(task.dimension):
                self._occupy(self.state_matrices[i], self._state_matrices_capacity[i], task.resources[i], task.duration, start_time)
            self.scheduled_tasks.append((task, self.timestep_counter+task.duration))
            return True

    def timestep(self):
        """Proceed to the next timestep."""
        self.timestep_counter += 1

        # update state matrices
        for i in range(self.dimension):
            temp = np.delete(self.state_matrices[i], (0), axis=0)
            temp = np.append(temp, np.array([[255 for x in range(temp.shape[1])]]), axis=0)
            self.state_matrices[i] = temp
        for i in range(self.dimension):
            self._state_matrices_capacity[i].pop(0)
            self._state_matrices_capacity[i].append(self.resources[i])

        # remove completed tasks
        indices = []
        for i in range(len(self.scheduled_tasks)):
            if self.timestep_counter >= self.scheduled_tasks[i][1]:
                indices.append(i)
        for i in sorted(indices, reverse=True):
            del self.scheduled_tasks[i]

    def summary(self, bg_shape=None):
        """State representation."""
        if self.dimension > 0:
            temp = self._expand(self.state_matrices[0], bg_shape)
            for i in range(1, self.dimension):
                temp = np.concatenate((temp, self._expand(self.state_matrices[i], bg_shape)), axis=1)
            return temp
        else:
            return None

    def utilization(self):
        """Calculate utilization."""
        return sum([collections.Counter(matrix.flatten()).get(0, 0) for matrix in self.state_matrices])/sum(self.resources)/self.duration

    def _satisfy(self, capacity_matrix, required_resources, required_duration):
        """Find the earliest time to schedule task with required resources for required duration."""
        p1 = 0
        p2 = 0
        duration_bound = min([len(capacity) for capacity in capacity_matrix])
        while p1 < duration_bound and p2 < required_duration:
            if False in [capacity_matrix[i][p1] >= required_resources[i] for i in range(len(required_resources))]:
                p1 += 1
                p2 = 0
            else:
                p1 += 1
                p2 += 1
        if p2 == required_duration:
            return p1 - required_duration
        else:
            return -1

    def _occupy(self, state_matrix, state_matrix_capacity, required_resource, required_duration, start_time):
        """Occupy state matrix slots for required resources and duration."""
        for i in range(start_time, start_time+required_duration):
            for j in range(required_resource):
                state_matrix[i, len(state_matrix[i])-state_matrix_capacity[i]+j] = 0
            state_matrix_capacity[i] = state_matrix_capacity[i] - required_resource

    def _expand(self, matrix, bg_shape=None):
        """Expand state matrix to specified background shape."""
        if bg_shape is not None and bg_shape[0] >= matrix.shape[0] and bg_shape[1] >= matrix.shape[1]:
            temp = matrix
            if bg_shape[0] > matrix.shape[0]:
                temp = np.concatenate((temp, np.full((bg_shape[0]-matrix.shape[0], matrix.shape[1]), 255, dtype=np.uint8)), axis=0)
            if bg_shape[1] > matrix.shape[1]:
                temp = np.concatenate((temp, np.full((bg_shape[0], bg_shape[1]-matrix.shape[1]), 255, dtype=np.uint8)), axis=1)
            return temp
        else:
            return matrix

    def __repr__(self):
        return 'Node(state_matrices={0}, label={1})'.format(self.state_matrices, self.label)


# End of node.py

# Start of plot.py
import os
import matplotlib.pyplot as plt

def node_plot(allActions):
    if not os.path.exists('__cache__/node_plot'):
        os.makedirs('__cache__/node_plot')
    node_counts = {}
    for action in allActions:
        node_label = action.node.label
        if node_label in node_counts:
            node_counts[node_label] += 1
        else:
            node_counts[node_label] = 1

    nodes = list(node_counts.keys())
    counts = [node_counts[node] for node in nodes]

    plt.bar(nodes, counts)
    plt.xlabel('Nodes')
    plt.ylabel('Number of Scheduled Tasks')
    plt.title('Scheduled Tasks in Each Node')
    # Save the plot to a file
    plt.savefig("__cache__/node_plot/Scheduled_Tasks.png")

    # Show the plot (if you want to display it)
    #plt.show()


'''
import numpy as np
def reward_plot(all_total_rewards):
    if not os.path.exists('__cache__/reward_plot'):
        os.makedirs('__cache__/reward_plot')
    for i in range(len(all_total_rewards)):
        all_total_rewards[i] = all_total_rewards[i]

    plt.plot(range(0, len(all_total_rewards)), all_total_rewards, marker='o')
    plt.xlabel("Episode")  # Modify x-axis label
    plt.ylabel("Task Slowdown")

    # Save the plot to a file
    plt.savefig("__cache__/reward_plot/all_reward.png")

    # Show the plot (if you want to display it)
    plt.show()


def d_rewards_plot(discounted_rewards):
    if not os.path.exists('__cache__/reward_plot'):
        os.makedirs('__cache__/reward_plot')
    # Calculate discounted rewards after collecting all rewards in the episode
    for i in range(len(discounted_rewards)):
        discounted_rewards[i] = discounted_rewards[i]

    plt.plot(range(0, len(discounted_rewards)), discounted_rewards, marker='o')
    plt.xlabel("Episode")
    plt.ylabel("Discounted_Total_Rewards")
    plt.savefig("__cache__/reward_plot/Discounted_Reward_Evolution_Over_Episodes.png")
    plt.show()
'''
'''
def reward_plots(all_r):
    if not os.path.exists('__cache__/reward_plot'):
        os.makedirs('__cache__/reward_plot')
    for i in range(len(all_r)):
        all_r[i] = all_r[i]

    plt.plot(range(0, len(all_r)), all_r, marker='o')
    plt.xlabel("Episode")  # Modify x-axis label
    plt.ylabel("Total reward-reset")
    plt.savefig("__cache__/reward_plot/Reward Evolution Over Episode.png")
    plt.show()
'''

'''

def d_rewards_plot(discounted_rewards):
    if not os.path.exists('__cache__/reward_plot'):
        os.makedirs('__cache__/reward_plot')
    for i in range(len(discounted_rewards)):
        # Calculate discounted rewards after collecting all rewards in the episode
        discounted_rewards[i] += 0.99 * discounted_rewards[i - 1]

    plt.plot(range(0, len(discounted_rewards)), discounted_rewards, marker='o')
    plt.xlabel("Episodes")  # Modify x-axis label
    plt.ylabel("Discounted_Rewards")
    plt.savefig("__cache__/reward_plot/Discounted_Reward Evolution Over Episodes.png")
    plt.show()


def reward_plots(all_r):
    for i in range(len(all_r)):
        all_r[i] = all_r[i]

    plt.plot(range(0, len(all_r)), all_r, marker='o')
    plt.xlabel("Iterations")  # Modify x-axis label
    plt.ylabel("Rewards")
    plt.show()


def node_plot(allActions):
        node_counts = {}
        for action in allActions:
            node_label = action.node.label
            if node_label in node_counts:
                node_counts[node_label] += 1
            else:
                node_counts[node_label] = 1

        nodes = list(node_counts.keys())
        counts = [node_counts[node] for node in nodes]

        plt.bar(nodes, counts)
        plt.xlabel('Node')
        plt.ylabel('Number of Scheduled Tasks')
        plt.title('Scheduled Tasks in Each Node')
        plt.show()
        print()


'''

# this is reward for each itration in each epesode 1000 itration in each epesode

'''


# this is plot nodes with tasks without numbers, showing the scheduling time in each node

def node_plots(allActions):
    node1 = []
    node2 = []
    node3 = []
    node4 = []
    node5 = []

    y1 = []
    y2 = []
    y3 = []
    y4 = []
    y5 = []
    for i in range(len(allActions)):

        if allActions[i].node.label == 'node1':
            node1.append(allActions[i].task.label)
            y1.append(i)
        elif allActions[i].node.label == 'node2':
            node2.append(allActions[i].task.label)
            y2.append(i)
        elif allActions[i].node.label == 'node3':
            node3.append(allActions[i].task.label)
            y3.append(i)
        elif allActions[i].node.label == 'node4':
            node4.append(allActions[i].task.label)
            y4.append(i)
        elif allActions[i].node.label == 'node5':
            node5.append(allActions[i].task.label)
            y5.append(i)

        print(allActions[i].node.label)

    #node1
    x1 = [1] * len(node1)
    plt.scatter(x1, y1)

    #node2
    x2 = [2] * len(node2)
    plt.scatter(x2, y2)

    #node3
    x3 = [3] * len(node3)
    plt.scatter(x3, y3)
    # node4
    x4 = [4] * len(node4)
    plt.scatter(x4, y4)
    # node5
    x5 = [5] * len(node5)
    plt.scatter(x5, y5)

    plt.title("Task Scheduling on Edge Nodes")  # Add title
    plt.xlabel("Tasks")  # Modify x-axis label
    plt.ylabel("Nodes")  # Modify y-axis label

    plt.show()
    print()'''

# End of plot.py

# Start of schedule.py
"""Schedulers."""

import datetime
import os
from abc import ABC, abstractmethod

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
import matplotlib.pyplot as plt
import env


class Action(object):
    """Schedule Action."""

    def __init__(self, task, node):
        self.task = task
        self.node = node

    def __repr__(self):
        return 'Action(task={0} -> node={1})'.format(self.task.label, self.node.label)


class Scheduler(ABC):
    """Scheduler Interface."""

    @abstractmethod
    def schedule(self):
        pass


class CompactScheduler(Scheduler):
    """Compact scheduler."""

    def __init__(self, environment):
        self.environment = environment

    def schedule(self):
        """Higher priority for higher utilization."""
        actions = []
        indices = []

        # sort nodes according to reversed utilization, schedule tasks from queue to nodes
        for i_task in range(len(self.environment.queue)):
            pairs = [(i_node, self.environment.nodes[i_node].utilization()) for i_node in range(len(self.environment.nodes))]
            pairs = sorted(pairs, key=lambda pair: pair[1], reverse=True)
            for pair in pairs:
                if self.environment.nodes[pair[0]].schedule(self.environment.queue[i_task]):
                    actions.append(Action(self.environment.queue[i_task], self.environment.nodes[pair[0]]))
                    indices.append(i_task)
                    break
        for i in sorted(indices, reverse=True):
            del self.environment.queue[i]

        # proceed to the next timestep
        self.environment.timestep()

        return actions
class SpreadScheduler(Scheduler):
    """Spread scheduler."""

    def __init__(self, environment):
        self.environment = environment

    def schedule(self):
        """Higher priority for lower utilization."""
        actions = []
        indices = []

        # sort nodes according to utilization, schedule tasks from queue to nodes
        for i_task in range(len(self.environment.queue)):
            pairs = [(i_node, self.environment.nodes[i_node].utilization()) for i_node in range(len(self.environment.nodes))]
            pairs = sorted(pairs, key=lambda pair: pair[1])
            for pair in pairs:
                if self.environment.nodes[pair[0]].schedule(self.environment.queue[i_task]):
                    actions.append(Action(self.environment.queue[i_task], self.environment.nodes[pair[0]]))
                    indices.append(i_task)
                    break
        for i in sorted(indices, reverse=True):
            del self.environment.queue[i]

        # proceed to the next timestep
        self.environment.timestep()

        return actions


class CNNModel(tf.keras.Model):
    """CNN Model."""

    def __init__(self, input_shape, output_shape):
        super(CNNModel, self).__init__()

        if os.path.isfile('__cache__/model/deeprm.keras'):
            self.model = tf.keras.models.load_model('__cache__/model/deeprm.keras')
        else:
            self.model = Sequential([
                Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=input_shape),
                MaxPooling2D(),
                Dropout(0.2),
                Flatten(),
                Dense(256, activation='relu'),
                Dense(output_shape, activation='linear')
            ])

        # Compile the model here
        self.model.compile(optimizer='adam', loss='mean_squared_error')  # Customize with your preferred optimizer and loss

    @tf.function
    def call(self, input_data):
        """Call model."""
        return self.model(input_data)

    def save(self):
        """Save model."""
        if not os.path.exists('__cache__/model'):
            os.makedirs('__cache__/model')
        self.model.save('__cache__/model/deeprm.keras')



class DQN(object):
    """DQN Implementation."""

    def __init__(self, input_shape, output_shape):
        self.lr = 0.01
        self.gamma = 0.99
        self.batch_size = 32
        self.min_experiences = 100
        self.max_experiences = 10000
        self.optimizer = tf.optimizers.Adam(self.lr)
        self.num_actions = output_shape
        self.model = CNNModel(input_shape, output_shape)
        self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []}

    def predict(self, input_data):
        """Predict Q value given state."""
        return self.model(input_data.astype('float32').reshape(input_data.shape[0], input_data.shape[1], input_data.shape[2], 1))

    @tf.function
    def train(self, dqn_target):
        """Train DQN."""
        if len(self.experience['s']) < self.min_experiences:
            return

        # samples
        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)
        states = np.asarray([self.experience['s'][i] for i in ids])
        actions = np.asarray([self.experience['a'][i] for i in ids])
        rewards = np.asarray([self.experience['r'][i] for i in ids])
        states_next = np.asarray([self.experience['s2'][i] for i in ids])
        dones = np.asarray([self.experience['done'][i] for i in ids])

        # use target model to calculate actual values
        values_next = np.max(dqn_target.predict(states_next), axis=1)
        actual_values = np.where(dones, rewards, rewards+self.gamma*values_next)

        # use train model to calculate predict values and loss
        with tf.GradientTape() as tape:
            predicted_values = tf.math.reduce_sum(self.predict(states) * tf.one_hot(actions, self.num_actions), axis=1)
            loss = tf.math.reduce_sum(tf.square(actual_values - predicted_values))

        # apply gradient descent to update train model
        variables = self.model.trainable_variables
        gradients = tape.gradient(loss, variables)
        self.optimizer.apply_gradients(zip(gradients, variables))

    def get_action(self, states, epsilon):
        """Predict action according to state (espilon for exploration)."""
        if np.random.random() < epsilon:
            return np.random.choice(self.num_actions)
        else:
            return np.argmax(self.predict(np.array([states]))[0])

    def add_experience(self, exp):
        """Add experience into replay buffer."""
        if len(self.experience['s']) >= self.max_experiences:
            for key in self.experience.keys():
                self.experience[key].pop(0)
        for key, value in exp.items():
            self.experience[key].append(value)

    def copy_weights(self, dqn_src):
        """Copy weights between models."""
        variables1 = self.model.trainable_variables
        variables2 = dqn_src.model.trainable_variables
        for v1, v2 in zip(variables1, variables2):
            v1.assign(v2.numpy())

    def save_weights(self):
        """Save model."""
        self.model.save()


class DeepRMTrainer(object):
    """DeepRM Trainer."""

    def __init__(self, environment):
        self.episodes = 3
        self.copy_steps = 32
        self.save_steps = 32
        self.epsilon = 0.99
        self.decay = 0.99
        self.min_epsilon = 0.1
        input_shape = (environment.summary().shape[0], environment.summary().shape[1], 1)
        output_shape = environment.queue_size * len(environment.nodes) + 1
        self.dqn_train = DQN(input_shape, output_shape)
        self.dqn_target = DQN(input_shape, output_shape)

        self.total_rewards = np.empty(self.episodes)
        self.task_delay = np.empty(self.episodes)

        self.cumulative_rewards_per_episode = []
        self.task_delay_per_episode = []

        self.environment = environment
        if not os.path.exists('__cache__/summary'):
            os.makedirs('__cache__/summary')
        self.summary_writer = tf.summary.create_file_writer('__cache__/summary/dqn-{0}'.format(datetime.datetime.now().strftime("%Y%m%d-%H%M%S")))

    def train(self):
        """Train process."""
        for i in range(self.episodes):
            self.epsilon = max(self.min_epsilon, self.epsilon*self.decay)

            self.total_rewards[i] = self.train_episode()
            self.task_delay[i] = -self.total_rewards[i]

            self.cumulative_rewards_per_episode.append(np.sum(self.total_rewards[i]))
            self.task_delay_per_episode.append(-self.total_rewards[i])

            with self.summary_writer.as_default():
                # task_delay is the negative total reward
                tf.summary.scalar('Episode Task Delay ', -self.total_rewards[i], step=i)
            print('Episode {0} Task Delay  {1}'.format(i, -self.total_rewards[i]))

        self.plot_metrics()

    def train_episode(self):
        """Train process of single episode."""
        rewards = 0
        step = 0
        self.environment, _ = env.load(load_scheduler=False)
        while not self.environment.terminated():
            # observe state and predict action
            observation = self.environment.summary()
            action_index = self.dqn_train.get_action(observation, self.epsilon)
            task_index, node_index = self._explain(action_index)

            # invalid action, proceed to the next timestep
            if task_index < 0 or node_index < 0:
                self.environment.timestep()
                continue
            scheduled_task = self.environment.queue[task_index]
            scheduled_node = self.environment.nodes[node_index]
            scheduled = scheduled_node.schedule(scheduled_task)
            if not scheduled:
                self.environment.timestep()
                continue

            # apply action, calculate reward and train model
            del self.environment.queue[task_index]
            prev_observation = observation
            reward = self.environment.reward()
            observation = self.environment.summary()
            rewards = rewards + reward
            exp = {'s': prev_observation, 'a': action_index, 'r': reward, 's2': observation, 'done': self.environment.terminated()}
            self.dqn_train.add_experience(exp)
            self.dqn_train.train(self.dqn_target)

            step += 1
            # copy weights from train model to target model periodically
            if step % self.copy_steps == 0:
                self.dqn_target.copy_weights(self.dqn_train)
            # save model periodically
            if step % self.save_steps == 0:
                self.dqn_target.save_weights()

        return rewards

    def plot_metrics(self):
        plt.figure(figsize=(10, 6))

        # Plot Total Rewards
        plt.plot(self.total_rewards, label='Total Rewards', color='blue')
        plt.title('Total Rewards Over Episodes')
        plt.xlabel('Episodes')
        plt.ylabel('Total Rewards')
        plt.legend()
        plt.grid(True)

        # Save the Total Rewards plot in the cache folder
        if not os.path.exists('__cache__/plots'):
            os.makedirs('__cache__/plots')
        plt.savefig('__cache__/plots/total_rewards_plot.png')

        # Create a new figure for task_delay
        plt.figure(figsize=(10, 6))

        # Plot task_delay
        plt.plot(self.task_delay, label='task_delay', color='red')
        plt.title('task_delay  Over Episodes')
        plt.xlabel('Episodes')
        plt.ylabel('task_delay')
        plt.legend()
        plt.grid(True)

        # Save the task_delay  plot in the cache folder
        plt.savefig('__cache__/plots/task_delay_plot.png')

        # Display the plots if needed
        #plt.show()

        with open('__cache__/metrics_data.csv', 'w') as file:
            file.write("Episode\tCumulative Rewards\ttask_delay\n")
            for i in range(self.episodes):
                file.write(f"{i}\t{self.cumulative_rewards_per_episode[i]}\t{self.task_delay_per_episode[i]}\n")
                
    def _explain(self, action_index):
        """Explain action."""
        task_limit = self.environment.queue_size
        node_limit = len(self.environment.nodes)
        if action_index == task_limit*node_limit:
            task_index = -1
            node_index = -1
        else:
            task_index = action_index % task_limit
            node_index = action_index // task_limit
        if task_index >= len(self.environment.queue):
            task_index = -1
            node_index = -1
        return (task_index, node_index)


class DeepRMScheduler(Scheduler):
    """DeepRM scheduler."""

    def __init__(self, environment, train=True):
        if train:
            DeepRMTrainer(environment).train()
        input_shape = (environment.summary().shape[0], environment.summary().shape[1], 1)
        output_shape = environment.queue_size * len(environment.nodes) + 1
        self.dqn_train = DQN(input_shape, output_shape)
        self.dqn_target = DQN(input_shape, output_shape)
        self.environment = environment

    def schedule(self):
        """Schedule with trained model."""
        actions = []
        rewards = []
        # apply actions until there's an invalid one
        while True:
            observation = self.environment.summary()
            action_index = self.dqn_train.get_action(observation, epsilon=0.1)  # Set epsilon to a low non-zero value
            task_index, node_index = self._explain(action_index)
            if task_index < 0 or node_index < 0:
               break
            scheduled_task = self.environment.queue[task_index]
            scheduled_node = self.environment.nodes[node_index]
            scheduled = scheduled_node.schedule(scheduled_task)
            rewards.append(self.environment.reward())
            if not scheduled:
                break
            del self.environment.queue[task_index]
            actions.append(Action(scheduled_task, scheduled_node))

        # proceed to the next timestep
        self.environment.timestep()

        return actions, rewards

    def _explain(self, action_index):
        """Explain action."""
        task_limit = self.environment.queue_size
        node_limit = len(self.environment.nodes)
        if action_index == task_limit*node_limit:
            task_index = -1
            node_index = -1
        else:
            task_index = action_index % task_limit
            node_index = action_index // task_limit
        if task_index >= len(self.environment.queue):
            task_index = -1
            node_index = -1
        return (task_index, node_index)

# End of schedule.py

# Start of __main__.py
"""Entrance."""
import env
from schedule import DeepRMScheduler as SchedulerTrain
from FL_AVG import combine_agents, distribute_agents
import pandas as pd
from plot import node_plot

def test_agent(test_environment, dqn_scheduler, runs_test=3):
    scores = []
    for i in range(runs_test):
        actions, score = dqn_scheduler.schedule()
        print(f'run_test number {i}', score)
        scores.append(score)
    return sum(scores) / len(scores)

if __name__ == '__main__':
    print('Start---')

    number_of_agents = 3
    environments, _ = zip(*[env.load() for _ in range(number_of_agents)])
    dqn_schedulers = [SchedulerTrain(environment) for environment in environments]

    global_environment, global_dqn_scheduler = env.load()
    single_environment, single_dqn_scheduler = env.load()

    scores_single_agent = []
    scores_global_agent = []
    
    # Aggregation and training loop
    for run in range(5):
        # Train each agent's DQN scheduler and collect scores
        local_scores = []
        for i, scheduler in enumerate(dqn_schedulers):
            print(f'Start training DQN scheduler {i}:')
            scheduler.train()
            score = test_agent(environments[i], scheduler)
            local_scores.append(score)
            print(f'The score of DQN scheduler {i}:', score)
        
        # Update global model using combined agents
        global_dqn_scheduler = combine_agents(global_dqn_scheduler, dqn_schedulers, local_scores)
        
        # Distribute the updated global model to each agent
        dqn_schedulers = distribute_agents(global_dqn_scheduler, dqn_schedulers)
        
        # Test and collect scores for both single and global DQN schedulers
        score_single = test_agent(single_environment, single_dqn_scheduler)
        scores_single_agent.append(score_single)
        
        score_global = test_agent(global_environment, global_dqn_scheduler)
        scores_global_agent.append(score_global)
        
        print(f"Run {run}: Single DQN Score: {score_single}, Global DQN Score: {score_global}")
    
    # Save the scores to CSV files
    pd.DataFrame({
        'Single Agent': scores_single_agent,
        'Global Agent': scores_global_agent
    }).to_csv('agent_scores_comparison.csv', index=False)

# End of __main__.py

# Start of task.py
"""Task Manipulation."""

import os

import numpy as np
from PIL import Image


class Task:
    """Task."""

    def __init__(self, resources, duration, label):
        self.resources = resources
        self.duration = duration
        self.label = label
        self.dimension = len(resources)

    def summary(self, bg_shape=None):
        """State representation."""
        if bg_shape is None:
            bg_shape = (self.duration, max(self.resources))
        if self.dimension > 0:
            state_matrices = [np.full(bg_shape, 255, dtype=np.uint8) for i in range(self.dimension)]
            for i in range(self.dimension):
                for row in range(self.duration):
                    for col in range(self.resources[i]):
                        state_matrices[i][row, col] = 0
            temp = state_matrices[0]
            for i in range(1, self.dimension):
                temp = np.concatenate((temp, state_matrices[i]), axis=1)
            return temp
        else:
            return None

    def __repr__(self):
        return 'Task(resources={0}, duration={1}, label={2})'.format(self.resources, self.duration, self.label)


# End of task.py

# Start of __init__.py

# End of __init__.py

# Start of activate_this.py
"""
Activate virtualenv for current interpreter:

Use exec(open(this_file).read(), {'__file__': this_file}).

This can be used when you must use an existing Python interpreter, not the virtualenv bin/python.
"""  # noqa: D415
from __future__ import annotations

import os
import site
import sys

try:
    abs_file = os.path.abspath(__file__)
except NameError as exc:
    msg = "You must use exec(open(this_file).read(), {'__file__': this_file}))"
    raise AssertionError(msg) from exc

bin_dir = os.path.dirname(abs_file)
base = bin_dir[: -len("Scripts") - 1]  # strip away the bin part from the __file__, plus the path separator

# prepend bin to PATH (this file is inside the bin directory)
os.environ["PATH"] = os.pathsep.join([bin_dir, *os.environ.get("PATH", "").split(os.pathsep)])
os.environ["VIRTUAL_ENV"] = base  # virtual env is right above bin directory
os.environ["VIRTUAL_ENV_PROMPT"] = "" or os.path.basename(base)  # noqa: SIM222

# add the virtual environments libraries to the host python import mechanism
prev_length = len(sys.path)
for lib in "..\\Lib\\site-packages".split(os.pathsep):
    path = os.path.realpath(os.path.join(bin_dir, lib))
    site.addsitedir(path.decode("utf-8") if "" else path)
sys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]

sys.real_prefix = sys.prefix
sys.prefix = base

# End of activate_this.py
